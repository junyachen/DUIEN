{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#!/data/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "#import model\n",
    "import random\n",
    "\n",
    "class Args():\n",
    "    is_training = False\n",
    "    layers = 1\n",
    "    rnn_size = 100  # RNN hidden unit num\n",
    "    n_epochs = 1  # number of epochs\n",
    "    batch_size = 512  # default 50\n",
    "    dropout_p_hidden = 1  # dropout probability in hidden layer\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.96  # used for batch norm\n",
    "    decay_steps = 1000\n",
    "    sigma = 0  # used for variable initial\n",
    "    init_as_normal = False\n",
    "    reset_after_session = True  # wheather resetting the hidden state when a session is finished\n",
    "    session_key = 'SessionId'  # column 1\n",
    "    item_key = 'ItemId'  # column 2\n",
    "    time_key = 'Time'  # column 3\n",
    "    grad_cap = 0\n",
    "    test_model = 1  # test model version\n",
    "    base_path = ''\n",
    "    checkpoint_dir = ''  # directory of check point\n",
    "    model_name = ''\n",
    "    loss = 'cross-entropy'\n",
    "    final_act = 'softmax'  # activation function of the final layer\n",
    "    hidden_act = 'tanh'  # activation function of the hidden layer\n",
    "    n_items = -1  # number of itemes\n",
    "    ori_file = ''  # ds of data\n",
    "    rnn_type = ''\n",
    "    is_preprocess = 0\n",
    "    DUIEN_layer = 2\n",
    "    head_num = 2\n",
    "    predict_sequence_length = 50\n",
    "    \n",
    "    attn_item_sbj_hidden = 8\n",
    "    \n",
    "    ## keep number of neighbors\n",
    "    max_degree = 100\n",
    "    enrich_seq_num = 2\n",
    "\n",
    "\n",
    "def parseArgs():\n",
    "    parser = argparse.ArgumentParser(description='DUIEN4Rec args')\n",
    "    parser.add_argument('--layer', default=1, type=int)  # default for single layer GRU\n",
    "    parser.add_argument('--size', default=128, type=int)  # rnn size(dimension)\n",
    "    parser.add_argument('--epoch', default=1, type=int)\n",
    "    parser.add_argument('--batch_size', default=256, type=int)\n",
    "    parser.add_argument('--lr', default=0.001, type=float)  # learning rate\n",
    "    parser.add_argument('--train', default=1, type=int)\n",
    "    parser.add_argument('--test', default=1, type=int)\n",
    "    parser.add_argument('--hidden_act', default='tanh', type=str)\n",
    "    parser.add_argument('--final_act', default='softmax', type=str)\n",
    "    parser.add_argument('--loss', default='cross-entropy', type=str)\n",
    "    parser.add_argument('--dropout', default='0.6', type=float)\n",
    "    parser.add_argument('--test_model', default=0, type=int)\n",
    "    parser.add_argument('--base_path', default='', type=str)\n",
    "    parser.add_argument('--ori_file', default='', type=str)\n",
    "    parser.add_argument('--model_name', default='', type=str)\n",
    "    parser.add_argument('--rnn_type', default='gru', type=str)\n",
    "    parser.add_argument('--decay_steps', default=1000, type=int)\n",
    "    parser.add_argument('--is_preprocess', default=0, type=int)\n",
    "    parser.add_argument('--predict_sequence_length', default=20, type=int)\n",
    "    parser.add_argument('--DUIEN_layer', default=2, type=int)\n",
    "    parser.add_argument('--head_num', default=2, type=int)\n",
    "    parser.add_argument('--attn_item_sbj_hidden', default=8, type=int)\n",
    "    parser.add_argument('--sliding_window', default=3, type=int)\n",
    "    parser.add_argument('--max_degree', default=100, type=int)\n",
    "    parser.add_argument('--enrich_seq_num', default=2, type=int)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy \n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[int(x)] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        results['accuracy'] = accuracy_score(Y, Y_)\n",
    "        \n",
    "        # print('Results, using embeddings of dimensionality', len(self.embeddings[X[0]]))\n",
    "        # print('-------------------')\n",
    "        #print(results)\n",
    "        return results\n",
    "        # print('-------------------')\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = numpy.asarray([self.embeddings[int(x)] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "    \n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['foo']\n",
    "command_line = parseArgs()       # get external parameters\n",
    "args = Args()\n",
    "\n",
    "args.layers = command_line.layer\n",
    "args.rnn_size = command_line.size\n",
    "args.n_epochs = command_line.epoch\n",
    "args.learning_rate = command_line.lr\n",
    "args.is_training = command_line.train\n",
    "args.test_model = command_line.test\n",
    "args.batch_size = command_line.batch_size\n",
    "args.hidden_act = command_line.hidden_act\n",
    "args.final_act = command_line.final_act\n",
    "args.loss = command_line.loss\n",
    "args.test_model = command_line.epoch - 1  # use the model saved by the last epoch\n",
    "args.dropout_p_hidden = 1.0 if args.is_training == 0 else command_line.dropout  # use dropout(0.5) in training phase\n",
    "args.base_path = command_line.base_path\n",
    "args.ori_file = command_line.ori_file\n",
    "args.checkpoint_dir = args.base_path + 'checkpoint_foursquare_DUIEN/' + args.ori_file\n",
    "args.model_name = command_line.model_name\n",
    "args.rnn_type = command_line.rnn_type\n",
    "args.decay_steps = command_line.decay_steps\n",
    "args.is_preprocess = command_line.is_preprocess\n",
    "args.predict_sequence_length = command_line.predict_sequence_length\n",
    "args.DUIEN_layer = command_line.DUIEN_layer\n",
    "args.head_num = command_line.head_num\n",
    "\n",
    "args.sliding_window = command_line.sliding_window\n",
    "\n",
    "args.savedModel = 1\n",
    "\n",
    "args.attn_item_sbj_hidden = command_line.attn_item_sbj_hidden\n",
    "args.max_degree = command_line.max_degree\n",
    "args.enrich_seq_num = command_line.enrich_seq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.base_path = \"datasets/\"\n",
    "args.ori_file = \"foursquare_seq\"\n",
    "#2020052812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.checkpoint_dir):\n",
    "        os.mkdir(args.checkpoint_dir)\n",
    "        with open(args.base_path + args.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('mkdir ' + args.checkpoint_dir)\n",
    "else:\n",
    "    with open(args.base_path + args.model_name + \".log\", \"a\") as log_file:\n",
    "        log_file.write('already exists: ' + args.checkpoint_dir)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"     # specify which GPU(s) to be used\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)    # determines the fraction of the overall amount of memory that each visible GPU can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# !/data/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "#import layers\n",
    "import random\n",
    "# from utils import *\n",
    "# from rnn import *\n",
    "import networkx as nx\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "class DUIEN4Rec:\n",
    "    def __init__(self, sess, args):\n",
    "        self.sess = sess\n",
    "        self.is_preprocess = args.is_preprocess\n",
    "\n",
    "        self.is_training = args.is_training\n",
    "        self.layers = args.layers\n",
    "        self.rnn_size = args.rnn_size\n",
    "        self.n_epochs = args.n_epochs\n",
    "\n",
    "        self.dropout_p_hidden = args.dropout_p_hidden\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.decay = args.decay  # default:0.96\n",
    "        self.decay_steps = args.decay_steps  # default:1e4\n",
    "        self.sigma = args.sigma  # param for initializer, default:0\n",
    "        self.init_as_normal = args.init_as_normal  # default:False\n",
    "        self.reset_after_session = args.reset_after_session  # default:True\n",
    "        self.session_key = args.session_key\n",
    "        self.item_key = args.item_key\n",
    "        self.time_key = args.time_key\n",
    "        self.grad_cap = args.grad_cap  # default:0\n",
    "        self.base_path = args.base_path\n",
    "        self.model_name = args.model_name\n",
    "        self.rnn_type = args.rnn_type\n",
    "        self.ori_file = args.ori_file\n",
    "        self.test_model = args.test_model\n",
    "        self.sequence_length = args.predict_sequence_length\n",
    "        self.max_click = 200\n",
    "\n",
    "        self.DUIEN_layer = args.DUIEN_layer\n",
    "        self.head_num = args.head_num\n",
    "        self.um_tag_length = 30\n",
    "        self.um_sbj_length = 30\n",
    "\n",
    "        self.train_path = self.base_path\n",
    "        self.batch_size_pos = args.batch_size\n",
    "        \n",
    "        self.attn_item_sbj_hidden = args.attn_item_sbj_hidden\n",
    "        \n",
    "        \n",
    "        self.max_degree = 100\n",
    "        self.tolerant_time = 40\n",
    "        \n",
    "        self.max_text_len = 30\n",
    "        #self.max_text_len = 20\n",
    "        \n",
    "        if self.is_preprocess == 1:\n",
    "            return\n",
    "\n",
    "        #self.time_emb_num = 150\n",
    "        self.time_emb_num = 30\n",
    "        self.short_emb_size = 4\n",
    "\n",
    "        if args.hidden_act == 'tanh':\n",
    "            self.hidden_act = self.tanh\n",
    "        elif args.hidden_act == 'relu':\n",
    "            self.hidden_act = self.relu\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # different loss function can try with different final_activation in training phase\n",
    "        if args.loss == 'cross-entropy':  # point-wise ranking loss\n",
    "            if args.final_act == 'tanh':\n",
    "                self.final_activation = self.softmaxth\n",
    "            else:\n",
    "                self.final_activation = self.softmax\n",
    "            self.loss_function = self.cross_entropy\n",
    "        elif args.loss == 'bpr':  # pair-wise ranking loss\n",
    "            if args.final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif args.final_act == 'relu':\n",
    "                self.final_activation = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.bpr\n",
    "        elif args.loss == 'top1':  # pair-wise ranking loss\n",
    "            if args.final_act == 'linear':\n",
    "                self.final_activation = self.linear\n",
    "            elif args.final_act == 'relu':\n",
    "                self.final_activatin = self.relu\n",
    "            else:\n",
    "                self.final_activation = self.tanh\n",
    "            self.loss_function = self.top1\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.checkpoint_dir = args.checkpoint_dir\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\"[!] Checkpoint Dir not found\")\n",
    "    \n",
    "    def init_model(self):\n",
    "    \n",
    "        self.n_items = max(self.ItemIdxList) + 2\n",
    "        self.n_tags = max(self.TagIdxList) + 2\n",
    "\n",
    "        self.predictTopN = self.n_items\n",
    "        # if max(self.ItemIdxList) > 100000:\n",
    "        #     self.predictTopN = 100000\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        print(\"init1\")\n",
    "        if self.is_training == 1:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        print(\"init2\")\n",
    "        #         self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=10)\n",
    "        self.saver = tf.train.Saver(max_to_keep=10)\n",
    "        print(\"init3\")\n",
    "\n",
    "        if self.is_training == 1:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # use self.predict_state to hold hidden states during prediction.\n",
    "        #         self.predict_state = [np.zeros([self.batch_size, self.rnn_size], dtype=np.float32) for _ in range(self.layers)]\n",
    "        print(\"init4\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, '{}/DUIEN-model-{}'.format(args.base_path + self.checkpoint_dir, self.test_model))\n",
    "        else:\n",
    "            print('no {}/DUIEN-model-{} !!!!!'.format(self.checkpoint_dir, self.test_model))\n",
    "        print(\"init5\")\n",
    "\n",
    "    ########################ACTIVATION FUNCTIONS#########################\n",
    "    def linear(self, X):\n",
    "        return X\n",
    "\n",
    "    def tanh(self, X):\n",
    "        return tf.nn.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        return tf.nn.softmax(X)\n",
    "\n",
    "    def softmaxth(self, X):\n",
    "        return tf.nn.softmax(tf.tanh(X))\n",
    "\n",
    "    def relu(self, X):\n",
    "        return tf.nn.relu(X)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return tf.nn.sigmoid(X)\n",
    "\n",
    "    ############################LOSS FUNCTIONS######################\n",
    "    def cross_entropy(self, yhat):  # yhat is a tensor with shape of [batch_size, batch_size]\n",
    "        return tf.reduce_mean(-tf.log(tf.diag_part(yhat) + 1e-24))\n",
    "\n",
    "    def corss_entropy_2(self, yhat):\n",
    "        return tf.reduce_mean(-tf.log(yhat[:, 0] + 1e-24))\n",
    "\n",
    "    def bpr_2(self, yhat):\n",
    "        yhat_pos = yhat[:, 0]\n",
    "        yhat_pos_column = tf.reshape(yhat_pos, [-1, 1])\n",
    "        return tf.reduce_mean(-tf.log(tf.nn.sigmoid(yhat_pos_column - yhat)))\n",
    "\n",
    "    def bpr(self, yhat):\n",
    "        yhatT = tf.transpose(yhat)\n",
    "        return tf.reduce_mean(-tf.log(tf.nn.sigmoid(tf.diag_part(yhat) - yhatT)))\n",
    "\n",
    "    def top1(self, yhat):\n",
    "        yhatT = tf.transpose(yhat)\n",
    "        term1 = tf.reduce_mean(tf.nn.sigmoid(-tf.diag_part(yhat) + yhatT) + tf.nn.sigmoid(yhatT ** 2), axis=0)\n",
    "        term2 = tf.nn.sigmoid(tf.diag_part(yhat) ** 2) / self.batch_size\n",
    "        return tf.reduce_mean(term1 - term2)  \n",
    "    \n",
    "    def build_transformer(self, layer_num, head_num, model_size, initializer):\n",
    "        W_TRFM = {}\n",
    "        for layer in range(layer_num):\n",
    "            for head in range(head_num):\n",
    "                # define qkv\n",
    "                variable_WQ = 'WQ_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WQ] = tf.get_variable(variable_WQ, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "                \n",
    "                variable_WK = 'WK_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WK] = tf.get_variable(variable_WK, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "                \n",
    "                variable_WV = 'WV_layer_' + str(layer) + '_head_' + str(head)\n",
    "                W_TRFM[variable_WV] = tf.get_variable(variable_WV, [model_size, model_size / head_num],\n",
    "                                                      initializer=initializer)\n",
    "                \n",
    "            # define ffn variables\n",
    "            variable_WFFN = 'WFFN1_layer_' + str(layer)\n",
    "            W_TRFM[variable_WFFN] = tf.get_variable(variable_WFFN, [model_size, model_size], initializer=initializer)\n",
    "\n",
    "            variable_BFFN = 'BFFN1_layer_' + str(layer)\n",
    "            W_TRFM[variable_BFFN] = tf.get_variable(variable_BFFN, [1, 1, model_size], initializer=initializer)\n",
    "\n",
    "       \n",
    "        return W_TRFM\n",
    "\n",
    "    def run_transformer(self, layer_num, head_num, model_size, input_trfm, W_TRFM, h_UM, embedding_tip_weights):\n",
    "        for layer in range(layer_num):\n",
    "            for head in range(head_num):\n",
    "                \n",
    "                if layer == 0:\n",
    "                    h_UM_pro = tf.matmul(h_UM, embedding_tip_weights)\n",
    "                    h_UM_expand = tf.expand_dims(h_UM_pro, 2)\n",
    "                    attn_item_UM_tmp = tf.nn.leaky_relu(tf.matmul(input_trfm, h_UM_expand))\n",
    "                    attn_item_UM = tf.nn.softmax(tf.matmul(attn_item_UM_tmp, \n",
    "                                            tf.transpose(h_UM_expand, perm=[0, 2, 1]))/ tf.sqrt(tf.cast(self.rnn_size, tf.float32)))\n",
    "                    input_trfm =  tf.add_n([input_trfm, attn_item_UM])  \n",
    "                \n",
    "                attn_Q = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WQ_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_K = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WK_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                attn_V = tf.nn.relu(\n",
    "                    tf.tensordot(input_trfm, W_TRFM['WV_layer_' + str(layer) + '_head_' + str(head)], axes=1))\n",
    "                \n",
    "#                 tmp_out = tf.matmul(attn_Q, tf.transpose(attn_K, perm=[0, 2, 1])) / tf.sqrt(tf.cast(model_size, tf.float32))\n",
    "#                 diag_vals = tf.ones_like(tmp_out[0, :, :])\n",
    "#                 tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() \n",
    "#                 masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(tmp_out)[0], 1, 1]) \n",
    "#                 paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "#                 tmp_out = tf.where(tf.equal(masks, 0), paddings, tmp_out)\n",
    "#                 tmp_out = tf.nn.softmax(tmp_out)\n",
    "#                 attn_out = tf.matmul(tmp_out, attn_V)\n",
    "                attn_out = tf.matmul(tf.nn.softmax(\n",
    "                    tf.matmul(attn_Q, tf.transpose(attn_K, perm=[0, 2, 1])) / tf.sqrt(tf.cast(model_size, tf.float32))),\n",
    "                    attn_V)\n",
    "                if head == 0:\n",
    "                    attn_concat = attn_out\n",
    "                else:\n",
    "                    attn_concat = tf.concat([attn_concat, attn_out], 2)\n",
    "\n",
    "            attn_add = attn_concat + input_trfm\n",
    "            attn_norm = tf.contrib.layers.layer_norm(attn_add, begin_norm_axis=1)  # [None, input_length, hidden_size]\n",
    "            attn_ffn = tf.nn.relu(tf.add(tf.tensordot(attn_norm, W_TRFM['WFFN1_layer_' + str(layer)], axes=1),\n",
    "                                         tf.tile(W_TRFM['BFFN1_layer_' + str(layer)],\n",
    "                                                 [self.batch_size, self.sequence_length, 1])))\n",
    "\n",
    "            attn_add2 = attn_norm + attn_ffn\n",
    "            attn_norm2 = tf.contrib.layers.layer_norm(attn_add2, begin_norm_axis=1)  # [None, input_length, hidden_size]\n",
    "            \n",
    "            #input_trfm_tmp = input_trfm\n",
    "            input_trfm = attn_norm2\n",
    "            #self.input_trfm = input_trfm\n",
    "            \n",
    "#             if layer == 0:\n",
    "#                 h_UM_pro = tf.matmul(h_UM, embedding_tip_weights)\n",
    "#                 h_UM_expand = tf.expand_dims(h_UM_pro, 2)\n",
    "#                 attn_item_UM_tmp = tf.nn.leaky_relu(tf.matmul(input_trfm, h_UM_expand))\n",
    "#                 attn_item_UM = tf.nn.softmax(tf.matmul(attn_item_UM_tmp, \n",
    "#                                         tf.transpose(h_UM_expand, perm=[0, 2, 1]))/ tf.sqrt(tf.cast(self.rnn_size, tf.float32)))\n",
    "#                 input_trfm =  tf.add_n([input_trfm, attn_item_UM])\n",
    "#                 #input_trfm = tf.multiply(input_trfm, attn_item_UM)\n",
    "#                 #input_trfm = tf.add_n([input_trfm, tf.nn.leaky_relu(tf.multiply(input_trfm, attn_item_UM))])\n",
    "            \n",
    "             \n",
    "            \n",
    "        return input_trfm\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        # doc id to doc index\n",
    "        self.item_id_hash_table = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int32,\n",
    "                                                                     default_value=-1, name=\"itemIdMap\",\n",
    "                                                                     checkpoint=True)\n",
    "        self.tag_id_hash_table = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int32, default_value=-1, name=\"tagIdMap\", checkpoint=True)\n",
    "\n",
    "        # doc index to doc id\n",
    "        self.item_id_hash_table_reverse = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.int64,\n",
    "                                                                             default_value=-1, name=\"itemIdMap\",\n",
    "                                                                             checkpoint=True)\n",
    "\n",
    "        self.input_item_id_tensor = tf.placeholder(tf.int64, [None, None], name='input_item_id_tensor')  # [batch_size, sequence_length]\n",
    "        self.input_tag_id_tensor = tf.placeholder(tf.int64, [None, None],name='input_tag_id_tensor')  # [batch_size, sequence_length]\n",
    " \n",
    "        self.X_time_diff = tf.placeholder(tf.int32, [None, None], name='input_time_diff')  # [batch_size, sequence_length]\n",
    "\n",
    "        self.target_item_id_tensor = tf.placeholder(tf.int64, [None], name='target_item_id_tensor')  # [batch_size]\n",
    "\n",
    "        self.X = self.item_id_hash_table.lookup(self.input_item_id_tensor)\n",
    "        self.X_tag = self.tag_id_hash_table.lookup(self.input_tag_id_tensor)\n",
    "        \n",
    "        self.UM_tip_id_tensor = tf.placeholder(tf.int64, [None,None], name='UM_tip_id_tensor')  \n",
    "        self.UM_tip_id = self.tag_id_hash_table.lookup(self.UM_tip_id_tensor)\n",
    "\n",
    "        self.Y = self.item_id_hash_table.lookup(self.target_item_id_tensor)\n",
    "\n",
    "        \n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)  # global step can not be trained.\n",
    "\n",
    "        with tf.variable_scope('DUIEN_layer'):\n",
    "            sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (self.n_items + self.rnn_size))\n",
    "            if self.init_as_normal:\n",
    "                initializer = tf.random_normal_initializer(mean=0, stddev=sigma)\n",
    "            else:\n",
    "                initializer = tf.random_uniform_initializer(minval=-sigma, maxval=sigma)  # for default\n",
    "\n",
    "            embedding = tf.get_variable('embedding', [self.n_items, self.rnn_size], initializer=initializer)\n",
    "            embedding_tag = tf.get_variable('embedding_tag', [self.n_tags, self.rnn_size], initializer=initializer)\n",
    "         \n",
    "            embedding_tip_weights = tf.get_variable('embedding_tip_weights', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            \n",
    "            #um_W = tf.get_variable('um_w', [self.rnn_size, self.rnn_size], initializer=initializer)\n",
    "            #um_b = tf.get_variable('um_b', [self.rnn_size], initializer=initializer)        \n",
    "            \n",
    "            embedding_pos = tf.get_variable('embedding_pos', [1, self.max_click, self.rnn_size], initializer=initializer)\n",
    "\n",
    "            feat_W = tf.get_variable('feat_w', [self.rnn_size * 2, self.rnn_size], initializer=initializer)\n",
    "            feat_b = tf.get_variable('feat_b', [self.rnn_size], initializer=initializer)\n",
    "            \n",
    "            text_W = tf.get_variable('text_W', [self.max_text_len, 1], initializer=initializer)\n",
    "            text_b = tf.get_variable('text_b', [1], initializer=initializer)\n",
    "            \n",
    "            um_W = tf.get_variable('um_W', [self.max_text_len, 1], initializer=initializer)\n",
    "            um_b = tf.get_variable('um_b', [1], initializer=initializer)\n",
    "            \n",
    "            behavior_um_W = tf.get_variable('behavior_um_W', [self.rnn_size * 2, self.rnn_size], initializer=initializer)\n",
    "            behavior_um_b = tf.get_variable('behavior_um_b', [self.rnn_size], initializer=initializer)\n",
    "\n",
    "            user_W = tf.get_variable('user_w', [self.rnn_size * 3, self.rnn_size ], initializer=initializer)\n",
    "            user_b = tf.get_variable('user_b', [self.rnn_size ], initializer=initializer)\n",
    "\n",
    "            embedding_time_diff = tf.get_variable('embedding_time_diff', [self.time_emb_num, self.rnn_size], initializer=initializer)\n",
    "\n",
    "            self.softmax_W = tf.get_variable('softmax_w', [self.n_items, self.rnn_size], initializer=initializer)\n",
    "            self.softmax_b = tf.get_variable('softmax_b', [self.n_items], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "            self.W_TRFM = self.build_transformer(self.DUIEN_layer, self.head_num, self.rnn_size, initializer)\n",
    "\n",
    "            #item_tag_W = tf.get_variable('item_tag_W', [self.rnn_size * 2, self.rnn_size ], initializer=initializer)\n",
    "            #item_tag_b = tf.get_variable('item_tag_b', [self.rnn_size ], initializer=initializer)\n",
    "            \n",
    "            ### tip conv\n",
    "            self.UM_tag_emb = tf.nn.embedding_lookup(embedding_tag, self.UM_tip_id + 1)\n",
    "            self.UM_tag_emb_re = tf.reshape(self.UM_tag_emb,[-1,self.max_text_len,self.rnn_size])\n",
    "            self.UM_tag_emb_re = tf.expand_dims(self.UM_tag_emb_re, -1)\n",
    "            \n",
    "            self.UM_W = tf.Variable(tf.truncated_normal([3, 10, 1, 1], stddev=0.3))\n",
    "            self.conv_UM = tf.nn.conv2d(self.UM_tag_emb_re, self.UM_W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            self.h_UM = self.conv_UM\n",
    "            self.h_UM = tf.squeeze(self.h_UM, -1)\n",
    "            self.h_UM = tf.transpose(self.h_UM, perm=[0, 2, 1])\n",
    "            self.h_UM = tf.matmul(self.h_UM, um_W)\n",
    "            self.h_UM = tf.add(self.h_UM, um_b)\n",
    "            \n",
    "            self.h_UM = tf.squeeze(self.h_UM, -1)\n",
    "            self.h_UM = tf.nn.tanh(self.h_UM)\n",
    "            \n",
    "            ## tag conv\n",
    "            #self.text_tag_embed = tf.Variable(tf.truncated_normal([len(self.TagIdxList), self.rnn_size], stddev=0.3))\n",
    "            #self.Text_a = tf.placeholder(tf.int32, [None, self.max_text_len], name='Ta')\n",
    "            #self.TA = tf.nn.embedding_lookup(self.text_tag_embed, self.Text_a)\n",
    "            \n",
    "            #self.X_tag = tf.reshape(self.X_tag,[-1,self.max_text_len])\n",
    "            self.TA = tf.nn.embedding_lookup(embedding_tag, self.X_tag + 1)\n",
    "            \n",
    "            #self.T_A = tf.expand_dims(self.TA, -1)\n",
    "            self.T_A = tf.reshape(self.TA,[-1,self.sequence_length,self.max_text_len,self.rnn_size])\n",
    "            \n",
    "            #self.T_A = self.TA\n",
    "            #self.W2 = tf.Variable(tf.truncated_normal([2, self.rnn_size, 1, 1], stddev=0.3))\n",
    "            self.W2 = tf.Variable(tf.truncated_normal([20, 5, self.rnn_size, self.rnn_size], stddev=0.3))\n",
    "            self.convA = tf.nn.conv2d(self.T_A, self.W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            #self.convA = tf.nn.leaky_relu(self.convA)\n",
    "            #self.hA = tf.nn.tanh(self.convA)\n",
    "            #self.hA = tf.nn.tanh(tf.squeeze(self.convA, -1))\n",
    "            \n",
    "            #self.hA = tf.squeeze(self.convA, -1)\n",
    "            self.hA = self.convA\n",
    "            #self.hA = tf.reshape(self.hA,[-1,self.sequence_length,self.max_text_len,self.rnn_size])\n",
    "            #self.hA = tf.nn.tanh(self.hA)\n",
    "            \n",
    "            self.hA = tf.transpose(self.hA, perm=[0, 1, 3, 2])\n",
    "            self.hA = tf.matmul(self.hA, text_W)\n",
    "            self.hA = tf.add(self.hA, text_b)\n",
    "            \n",
    "            self.hA = tf.squeeze(self.hA, -1)\n",
    "            self.hA = tf.nn.tanh(self.hA)\n",
    "            ###\n",
    "            \n",
    "            \n",
    "            ### input process\n",
    "            self.inputs_item = tf.nn.embedding_lookup(embedding, self.X + 1)  # [batch_size, sequence_length, rnn_size]\n",
    "            #inputs_tag = tf.nn.embedding_lookup(embedding_tag, self.X_tag + 1)  #\n",
    "\n",
    "            # [batch_size, sequence_length, rnn_size * 2]\n",
    "            #inputs_feat = tf.concat([self.inputs_item, inputs_tag], 2)\n",
    "            inputs_feat = tf.concat([self.inputs_item, self.hA], 2)\n",
    "            #inputs_feat = tf.concat([self.inputs_item, self.inputs_item], 2)\n",
    "            #inputs_feat = tf.nn.leaky_relu(tf.add(tf.matmul(inputs_feat, item_tag_W), item_tag_b)) \n",
    "            #inputs_feat = self.inputs_item\n",
    "            \n",
    "            # [batch_size, sequence_length, rnn_size]\n",
    "            self.inputs_time_diff = tf.nn.embedding_lookup(embedding_time_diff, self.X_time_diff + 1)\n",
    "            self.inputs_tmp = tf.nn.tanh( tf.add(tf.tensordot(inputs_feat, feat_W, axes=1), feat_b)) + self.inputs_time_diff\n",
    "\n",
    "            self.inputs = tf.reshape(self.inputs_tmp, [self.batch_size, -1, self.rnn_size])\n",
    "\n",
    "            embedding_pos_slice = tf.slice(embedding_pos, [0, 0, 0], [1, self.sequence_length, self.rnn_size])\n",
    "            embedding_pos_rev = tf.reverse(embedding_pos_slice, [1])\n",
    "\n",
    "            self.inputs_pe = tf.add(self.inputs, tf.tile(embedding_pos_rev, [self.batch_size, 1, 1]))\n",
    "\n",
    "            self.trfm_input_um = self.inputs_pe\n",
    "\n",
    "            self.trfm_out  = self.run_transformer(self.DUIEN_layer, self.head_num, self.rnn_size, self.trfm_input_um, self.W_TRFM, self.h_UM, embedding_tip_weights)\n",
    "            #self.trfm_tip_out = tf.concat([self.trfm_out, self.h_UM], 2)\n",
    "            #self.trfm_out = tf.nn.leaky_relu(tf.add(tf.matmul(self.trfm_tip_out, behavior_um_W), behavior_um_b))\n",
    "            #self.trfm_out   = tf.nn.dropout(self.trfm_out, keep_prob=self.dropout_p_hidden)\n",
    "            \n",
    "            ## attn user interest\n",
    "            #self.tmp_trfm_out = self.trfm_out\n",
    "#             self.h_UM_pro = tf.matmul(self.h_UM, embedding_tip_weights)\n",
    "#             self.h_UM_expand = tf.expand_dims(self.h_UM_pro, 2)\n",
    "#             #self.attn_item_UM_tmp = tf.nn.leaky_relu(tf.matmul(self.trfm_out, self.h_UM_expand), alpha=0.2)\n",
    "#             self.attn_item_UM_tmp = tf.nn.leaky_relu(tf.matmul(self.trfm_out, self.h_UM_expand))\n",
    "#             self.attn_item_UM = tf.nn.softmax(tf.matmul(self.attn_item_UM_tmp, \n",
    "#                                     tf.transpose(self.h_UM_expand, perm=[0, 2, 1]))/ tf.sqrt(tf.cast(self.rnn_size, tf.float32)))\n",
    "#             self.trfm_out =  tf.add_n([self.trfm_out, self.attn_item_UM])\n",
    "            ###\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.trfm_final = tf.slice(self.trfm_out, [0, self.sequence_length - 2, 0], [self.batch_size, 2, self.rnn_size])\n",
    "            self.final_state = tf.reshape(self.trfm_final, [self.batch_size, -1])\n",
    "            # self.final_state = tf.squeeze(self.trfm_final)\n",
    "\n",
    "        self.user_vec1 = tf.reshape(self.final_state, [self.batch_size, -1])\n",
    "        \n",
    "        ### user concat\n",
    "        self.user_vec1 = tf.concat([self.user_vec1, self.h_UM], 1)\n",
    "        #self.user_final = tf.nn.leaky_relu(tf.add(tf.matmul(self.user_vec1, user_W), user_b))\n",
    "        #self.user_final = tf.add(tf.matmul(self.user_vec1, user_W), user_b)\n",
    "        self.user_final = tf.matmul(self.user_vec1, user_W)\n",
    "\n",
    "        if self.is_training == 1:\n",
    "            sampled_W = tf.nn.embedding_lookup(self.softmax_W, self.Y + 1)\n",
    "            sampled_b = tf.nn.embedding_lookup(self.softmax_b, self.Y + 1)\n",
    "            self.logits = tf.matmul(self.user_final, sampled_W, transpose_b=True) + sampled_b\n",
    "            self.yhat = self.final_activation(self.logits)\n",
    "            self.cost = self.loss_function(self.yhat)\n",
    "        #             self.prob = tf.nn.sigmoid(self.yhat)\n",
    "        else:\n",
    "            softmax_W_topN = tf.slice(self.softmax_W, [0, 0], [self.predictTopN, self.rnn_size])\n",
    "            softmax_b_topN = tf.slice(self.softmax_b, [0], [self.predictTopN])\n",
    "            \n",
    "            self.logits = tf.matmul(self.user_final, softmax_W_topN, transpose_b=True) + softmax_b_topN\n",
    "            # self.logits = tf.matmul(self.user_vec2, softmax_W, transpose_b=True) + softmax_b\n",
    "            self.yhat = self.final_activation(self.logits)\n",
    "            \n",
    "            # ## savedModel ## #\n",
    "            self.yhat = tf.identity(self.yhat, name=\"yhat\")\n",
    "\n",
    "            # topNum = tf.divide(tf.constant(1000), tf.max(tf.constant(1), outSeqNum))\n",
    "            self.topk_prob_sample, self.topk_ind_sample = tf.nn.top_k(self.yhat, 50, name=\"topk\")\n",
    "\n",
    "            self.topk_ind_sample = self.topk_ind_sample - 1\n",
    "\n",
    "            self.topk_doc_id = self.item_id_hash_table_reverse.lookup(tf.cast(self.topk_ind_sample, tf.int64))\n",
    "\n",
    "            topk_prob_sample_1d = tf.reshape(self.topk_prob_sample, [-1])\n",
    "            topk_ind_sample_1d = tf.reshape(self.topk_ind_sample, [-1])\n",
    "            topk_doc_id_1d = tf.reshape(self.topk_doc_id, [-1])\n",
    "\n",
    "            self.topk_ind_val = tf.add(topk_ind_sample_1d, tf.constant(0), name=\"topk_ind_val\")\n",
    "            self.topk_prob_val = tf.add(topk_prob_sample_1d, tf.constant(0.0), name=\"topk_prob_val\")\n",
    "            self.topk_doc_id = tf.identity(topk_doc_id_1d, name=\"topk_doc_id\")\n",
    "            return\n",
    "\n",
    "        self.lr = tf.maximum(1e-5, tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\n",
    "                                                              self.decay, staircase=True))\n",
    "\n",
    "        ''' Try different optimizers. '''\n",
    "        # optimizer = tf.train.AdagradOptimizer(self.lr)\n",
    "        # optimizer = tf.train.AdadeltaOptimizer(self.lr)\n",
    "        # optimizer = tf.train.RMSPropOptimizer(self.lr)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        gvs = optimizer.compute_gradients(self.cost, tvars)\n",
    "        if self.grad_cap > 0:\n",
    "            capped_gvs = [(tf.clip_by_norm(grad, self.grad_cap), var) for grad, var in gvs]\n",
    "        else:\n",
    "            capped_gvs = gvs\n",
    "        self.train_op = optimizer.apply_gradients(capped_gvs, global_step=self.global_step)\n",
    "        \n",
    "    def save_obj(self, path, obj, name):\n",
    "        with open(path + name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_obj(self, path, name):\n",
    "        with open(path + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "            \n",
    "    def preprocess(self, verbose=False):\n",
    "\n",
    "        \n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write( \"{} begin preprocessing...\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())) + '\\n')\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('start loading file: train_file: ' + self.train_path + self.ori_file + '\\n')\n",
    "\n",
    "        #         try:\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('loading raw data. \\n')\n",
    "            \n",
    "        \n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('finish loading raw data. \\n')\n",
    "\n",
    "\n",
    "        # get expand data\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting expand data. \\n')\n",
    "        \n",
    "        mv_data_pd = pd.read_pickle(self.base_path + self.ori_file+\".pkl\")\n",
    "        mv_data_pd['Sequence'] = mv_data_pd['Sequence'].astype('str')\n",
    "        #data_expand = pd.concat([Series(row['SessionId'], row['Sequence'].split('#')) for _, row in\n",
    "        #                         mv_data_pd.iterrows()]).reset_index()\n",
    "                                 \n",
    "        #data_expand.columns = ['ItemInfo', 'SessionId']\n",
    "        data_expand1 = pd.concat([Series(row['SessionId'], row['Sequence'].split('#')) \n",
    "                         for _, row in mv_data_pd.iterrows()]).reset_index()\n",
    "        data_expand2 = pd.concat([Series(row['tips'], row['Sequence'].split('#')) \n",
    "                                 for _, row in mv_data_pd.iterrows()]).reset_index()\n",
    "        \n",
    "        data_expand1.columns = ['ItemInfo', 'SessionId']\n",
    "        data_expand2.columns = ['ItemInfo', 'tips']\n",
    "        \n",
    "        #data_expand = pd.merge(data_expand1, data_expand2, on='ItemInfo', how='inner')\n",
    "        #data_expand.columns = ['ItemInfo', 'SessionId', 'tips']\n",
    "        data_expand = pd.concat([data_expand1, data_expand2], axis=1).reindex(data_expand1.index)\n",
    "        data_expand.columns = ['ItemInfo', 'SessionId', 'ItemInfo1', 'tips']\n",
    "        data_expand.drop(['ItemInfo1'], axis=1)\n",
    "        \n",
    "        data_expand[['ItemId', 'Time' ]] = pd.DataFrame( data_expand.ItemInfo.str.split('_').tolist())\n",
    "\n",
    "        # get item, tag, subject id-idx dictionary dataframe\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting item, tag, subject id-idx dictionary. \\n')\n",
    "\n",
    "        ItemIdCount = data_expand['ItemId'].value_counts()\n",
    "        ItemIdFreq = ItemIdCount.keys().tolist()\n",
    "        ItemIdFreqPd = pd.DataFrame(ItemIdFreq)\n",
    "        ItemIdFreqPd.columns = ['ItemId']\n",
    "        ItemIdPd = ItemIdFreqPd\n",
    "        ItemIdPd['ItemIdx'] = range(len(ItemIdFreq))\n",
    "        ItemIdPd.columns = ['ItemId', 'ItemIdx']\n",
    "\n",
    "        data_ItemInfo = data_expand[['ItemId']]\n",
    "        data_ItemInfo.drop_duplicates(inplace=True)\n",
    "        # data_ItemInfo = pd.merge(data_ItemInfo, ItemIdFreqPd, on='ItemId', how='inner')\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting feature data. \\n')\n",
    "            \n",
    "            \n",
    "        # get feature data\n",
    "        data_ItemInfo.dropna(inplace=True)\n",
    "        \n",
    "        data = data_expand[['SessionId', 'ItemId', 'Time', 'tips']]\n",
    "        data = pd.merge(data, data_ItemInfo, on='ItemId', how='inner')\n",
    "        \n",
    "        # add tag info\n",
    "        mv_data_tag_pd = pd.read_pickle(self.base_path + self.ori_file + \"_tags.pkl\")\n",
    "        data = pd.merge(data, mv_data_tag_pd, on='ItemId', how='inner')\n",
    "        \n",
    "        \n",
    "            \n",
    "        data = data[data['ItemId'] != '']\n",
    "        data = data[data['Time'] != '']\n",
    "        \n",
    "        data.ItemId = data.ItemId.astype(int)\n",
    "        data.Time = data.Time.astype(int)\n",
    "        data = data.sort_values(by=['SessionId', 'Time'])\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('getting offset_sessions. \\n')\n",
    "\n",
    "        # get offset_sessions\n",
    "        offset_sessions = np.zeros(data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "        offset_sessions[1:] = data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "            log_file.write('saving data by pickle. \\n')\n",
    "\n",
    "        # save obj\n",
    "        \n",
    "        self.save_obj(self.train_path, offset_sessions, self.ori_file + '_offset_sessions')\n",
    "        self.save_obj(self.train_path, data, self.ori_file + '_data')\n",
    "        self.save_obj(self.train_path, ItemIdPd, self.ori_file + '_ItemIdPd')\n",
    "        \n",
    "    \n",
    "    def enrich_edges(self):\n",
    "        G_item_item = nx.read_edgelist(self.train_path + self.ori_file + '_item_item_edge_list.txt', nodetype=int)\n",
    "        G_tag_tag = nx.read_edgelist(self.train_path + self.ori_file + '_tag_tag_edge_list.txt', nodetype=int)\n",
    "        G_item_tag = nx.read_edgelist(self.train_path + self.ori_file + '_item_tag_edge_list.txt', nodetype=int)\n",
    "        \n",
    "        data = self.load_obj(self.train_path, self.ori_file + '_data')\n",
    "        \n",
    "        Set_item_id = set(data['ItemId'].tolist())\n",
    "        Set_item_idx = {}\n",
    "        Set_item_idx_reverse = {}\n",
    "        Set_tag_id = set(data['TagId'].tolist())\n",
    "        Set_tag_idx = {}\n",
    "        Set_tag_idx_reverse = {}\n",
    "\n",
    "        for i,j in enumerate(Set_item_id):\n",
    "            Set_item_idx[j] = i\n",
    "            Set_item_idx_reverse[i] = j\n",
    "        for i,j in enumerate(Set_tag_id):\n",
    "            Set_tag_idx[j] = i\n",
    "            Set_tag_idx_reverse[i] = j\n",
    "            \n",
    "        seq_adj_item_item = np.ones((len(Set_item_id), self.max_degree))\n",
    "        for node_id in G_item_item.nodes():\n",
    "            neighbors = np.array( [Set_item_idx[i] for i in list(G_item_item.neighbors(node_id))] )\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            seq_adj_item_item[Set_item_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_tag_tag = np.ones((len(Set_tag_id), self.max_degree))\n",
    "        for node_id in G_tag_tag.nodes():\n",
    "            neighbors = np.array( [Set_tag_idx[i] for i in list(G_tag_tag.neighbors(node_id))] )\n",
    "            \n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
    "            seq_adj_tag_tag[Set_tag_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_item_tag = np.ones((len(Set_item_id), DUIEN.max_degree))\n",
    "        for node_id in G_item_item.nodes():\n",
    "            neighbors = np.array( [Set_tag_idx[int(i.split(\"_\")[0])] for i in list(G_item_tag.neighbors(str(node_id)))] )\n",
    "            if len(neighbors) > DUIEN.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, DUIEN.max_degree, replace=False)\n",
    "            elif len(neighbors) < DUIEN.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, DUIEN.max_degree, replace=True)\n",
    "            seq_adj_item_tag[Set_item_idx[node_id],:] = neighbors\n",
    "            \n",
    "        seq_adj_tag_item = np.ones((len(Set_tag_id), DUIEN.max_degree))\n",
    "        for node_id in G_tag_tag.nodes():\n",
    "            neighbors = np.array( [Set_item_idx[int(i)] for i in list(G_item_tag.neighbors(str(node_id)+\"_tag\"))] )\n",
    "            if len(neighbors) > DUIEN.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, DUIEN.max_degree, replace=False)\n",
    "            elif len(neighbors) < DUIEN.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, DUIEN.max_degree, replace=True)\n",
    "            seq_adj_tag_item[Set_tag_idx[node_id],:] = neighbors\n",
    "          \n",
    "\n",
    "        \n",
    "            \n",
    "        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('enrich meta path edgelists. \\n')\n",
    "                \n",
    "        print('enrich meta path edgelists')\n",
    "                \n",
    "        with open(self.train_path + self.ori_file + '_item_item_edge_list.txt', 'a') as f:\n",
    "            for item in G_item_item.nodes():\n",
    "                for item_i in range(G_item_item.degree(item)):\n",
    "                    random_tag_id = Set_tag_idx_reverse[random.choice(seq_adj_item_tag[Set_item_idx[item]])]\n",
    "                    random_item_id = Set_item_idx_reverse[random.choice(seq_adj_tag_item[Set_tag_idx[random_tag_id]] )]\n",
    "                    f.write(\"{} {}\\n\".format(str(item_i),str(random_item_id)))\n",
    "            f.close()\n",
    "        \n",
    "        with open(self.train_path + self.ori_file + '_tag_tag_edge_list.txt', 'a') as f:\n",
    "            for tag in G_tag_tag.nodes():\n",
    "                for tag_i in range(G_tag_tag.degree(tag)):\n",
    "                    random_item_id = Set_item_idx_reverse[random.choice(seq_adj_tag_item[Set_tag_idx[tag]] )]\n",
    "                    random_tag_id = Set_tag_idx_reverse[random.choice(seq_adj_item_tag[Set_item_idx[random_item_id]] )]\n",
    "                    f.write(\"{} {}\\n\".format(str(tag),str(random_tag_id)))\n",
    "            f.close()\n",
    "            \n",
    "        \n",
    "\n",
    "    def loaddata(self, verbose=False):\n",
    "        # load obj\n",
    "        print('loading data by pickle. \\n')\n",
    "\n",
    "        train_path = self.train_path\n",
    "        ori_file = self.ori_file\n",
    "        self.offset_sessions = self.load_obj(train_path, ori_file + '_offset_sessions')\n",
    "        self.data = self.load_obj(train_path, ori_file + '_data')\n",
    "        self.ItemIdPd = self.load_obj(train_path, ori_file + '_ItemIdPd')\n",
    "\n",
    "        self.ItemIdPd = self.ItemIdPd[self.ItemIdPd['ItemId'] != '']\n",
    "\n",
    "        self.ItemIdPd.dropna(inplace=True)\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        self.ItemIdPd = self.ItemIdPd[self.ItemIdPd['ItemId'] != 'nan']\n",
    "\n",
    "        self.ItemIdList = [int(k) for k in self.ItemIdPd.ItemId.tolist()]\n",
    "        self.ItemIdxList = [int(k) for k in self.ItemIdPd.ItemIdx.tolist()]\n",
    "\n",
    "        tags_set = set()\n",
    "        for tags in self.data.tags.tolist():\n",
    "            for i in tags.split(' '):\n",
    "                tags_set.add(i)\n",
    "        self.TagIdxList = range(len(tags_set))\n",
    "        \n",
    "\n",
    "        #test_cut = int(self.data.shape[0] * 1 / 2)\n",
    "        #self.data = self.data.iloc[test_cut:-1]\n",
    "        #self.data_test = self.data.iloc[0:test_cut]\n",
    "\n",
    "    def toFloat(self, inVal):\n",
    "        try:\n",
    "            outVal = float(inVal)\n",
    "        except:\n",
    "            outVal = 0.0\n",
    "\n",
    "        if np.isnan(outVal):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return outVal\n",
    "    def test(self):\n",
    "        \n",
    "        def sigmoid(input_val):\n",
    "            return 1 / (1 + (math.e ** -input_val))\n",
    "\n",
    "        self.error_during_train = False\n",
    "        self.sess.run(self.item_id_hash_table.insert(self.ItemIdList, self.ItemIdxList))\n",
    "        self.sess.run(self.item_id_hash_table_reverse.insert(self.ItemIdxList, self.ItemIdList))\n",
    "        \n",
    "        \n",
    "        max_test_auc = 0\n",
    "\n",
    "        self.offset_sessions = np.zeros(self.data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "        self.offset_sessions[1:] = self.data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "        cost_train_list = []\n",
    "        cost_test_list = []\n",
    "\n",
    "        auc_train_list = []\n",
    "        auc_test_list = []\n",
    "\n",
    "        session_idx_arr = np.arange(len(self.offset_sessions) - 2)\n",
    "        \n",
    "        self.HR_10  = 0\n",
    "        self.HR_5  = 0\n",
    "        self.HR_1  = 0\n",
    "        \n",
    "        self.MRR = 0\n",
    "        \n",
    "        for s_idx in session_idx_arr:\n",
    "            start = self.offset_sessions[s_idx]\n",
    "            end = self.offset_sessions[s_idx + 1]\n",
    "            if end - start > 1:\n",
    "                #pos_idx = end - 1\n",
    "                if start == end-2:\n",
    "                    pos_idx = [start]\n",
    "                else:\n",
    "                    pos_idx = np.arange(start, end-2)\n",
    "                \n",
    "                self.in_idx = np.array(self.data.ItemId.values[pos_idx].tolist())\n",
    "                self.in_tag = self.data.tags.values[pos_idx].tolist()\n",
    "                self.in_tag_list = np.asarray([in_tag_i.split(' ') for in_tag_i in self.in_tag])\n",
    "                self.in_tag = self.in_tag_list.astype(np.int)\n",
    "                \n",
    "                self.out_idx = np.array(self.data.ItemId.values[end-1])\n",
    "                #self.out_idx = np.array(np.ones((len(pos_idx))) * self.data.ItemId.values[end])\n",
    "                \n",
    "                \n",
    "                self.in_um_tip_id_list = [self.data['tips'][posid] for posid in pos_idx]\n",
    "                self.in_um_tip_id = np.asarray([in_tip_i.split(' ') for in_tip_i in self.in_um_tip_id_list])[0].reshape((1,-1))\n",
    "                self.in_um_tip_id = self.in_um_tip_id.astype(np.int)\n",
    "                \n",
    "                self.in_time = self.data.Time.values[pos_idx].tolist()\n",
    "                self.out_time = (np.ones((len(pos_idx))) * self.data.Time.values[end]).tolist()\n",
    "                \n",
    "                #self.in_time_diff1 = [self.out_time - self.in_time]\n",
    "                self.in_time_diff1 = [self.out_time[k] - self.in_time[k] for k in range(len(self.out_time))]\n",
    "                self.in_time_diff2 = [int(k % 30) for k in self.in_time_diff1]\n",
    "                self.in_time_diff3 = [min(28, k) for k in self.in_time_diff2]\n",
    "                self.in_time_diff = [max(0, k) for k in self.in_time_diff3]\n",
    "                \n",
    "                #self.in_idx_matrix = np.reshape(np.array(in_idx), [-1, 1])\n",
    "                #self.in_time_diff_matrix = np.reshape(np.array(self.in_time_diff), [-1, 1]) \n",
    "            \n",
    "                self.in_idx_matrix = np.zeros([1, self.sequence_length], dtype=int)\n",
    "                self.in_time_diff_matrix = np.zeros([1, self.sequence_length], dtype=int) + 28\n",
    "                \n",
    "                self.in_idx_matrix = np.concatenate([np.reshape(np.array(self.in_idx), [1, -1]), self.in_idx_matrix[:, 0:-1]], 1)\n",
    "                self.in_time_diff_matrix = np.concatenate([np.reshape(np.array(self.in_time_diff),\n",
    "                                                                     [1, -1]), self.in_time_diff_matrix[:, 0:-1]], 1)\n",
    "                \n",
    "                self.in_idx_matrix = self.in_idx_matrix[:,0:self.sequence_length]\n",
    "                self.in_time_diff_matrix = self.in_time_diff_matrix[:,0:self.sequence_length]\n",
    "                \n",
    "                self.in_tag_matrix = np.zeros([1, self.sequence_length * self.max_text_len], dtype=int)\n",
    "                \n",
    "                self.max_top_tag = 20\n",
    "                if self.in_tag.shape[0] > self.max_top_tag:\n",
    "                    self.top_tag = 20\n",
    "                else:\n",
    "                    self.top_tag = self.in_tag.shape[0]\n",
    "                \n",
    "                self.in_tag_matrix = np.concatenate([np.reshape(np.array(self.in_tag[0:self.top_tag,:]), [1, -1]), \n",
    "                                                     self.in_tag_matrix[:,0:-(self.top_tag*self.max_text_len)]], 1)\n",
    "                \n",
    "                def flip_swap_zero(matrix_in, fillval=-1):\n",
    "                        matrix_2d = []\n",
    "                        for batch_cnt in range(matrix_in.shape[0]):\n",
    "                            tmp_val = matrix_in[batch_cnt]\n",
    "                            tmp_val = np.flip(tmp_val[np.nonzero(tmp_val - fillval)], 0)\n",
    "                            matrix_2d.append(np.concatenate(\n",
    "                                [tmp_val, np.array([fillval] * (len(matrix_in[batch_cnt]) - len(tmp_val)))], 0))\n",
    "                        return matrix_2d\n",
    "\n",
    "                self.in_idx_matrix_swap = flip_swap_zero(self.in_idx_matrix)\n",
    "                self.in_time_diff_matrix_swap = flip_swap_zero(self.in_time_diff_matrix, fillval=28)\n",
    "                \n",
    "                self.in_tag_matrix_swap = flip_swap_zero(self.in_tag_matrix)\n",
    "\n",
    "        \n",
    "                feed_dict_test = {self.input_item_id_tensor: np.nan_to_num(self.in_idx_matrix_swap),\n",
    "                                  self.input_tag_id_tensor: np.nan_to_num(self.in_tag_matrix_swap),\n",
    "                                  self.X_time_diff: np.nan_to_num(self.in_time_diff_matrix_swap),\n",
    "                                  self.UM_tip_id_tensor: np.nan_to_num(np.array(self.in_um_tip_id)),\n",
    "                                  #self.target_item_id_tensor: [np.nan_to_num(np.array(self.out_idx))],\n",
    "\n",
    "                                  \n",
    "                                  self.batch_size: 1\n",
    "                                  }\n",
    "                \n",
    "                \n",
    "                fetches_test = [self._topk_ind_val, self._topk_prob_val, self._topk_doc_id]\n",
    "                self.topk_ind, self.topk_prob, self.topk_doc = self.sess.run(fetches_test, feed_dict_test)\n",
    "                \n",
    "                if self.out_idx  in self.topk_doc[0:10]:\n",
    "                    self.HR_10 += 1\n",
    "                    \n",
    "                if self.out_idx  in self.topk_doc[0:5]:\n",
    "                    self.HR_5 += 1\n",
    "                    \n",
    "                if self.out_idx  in self.topk_doc[0:1]:\n",
    "                    self.HR_1 += 1\n",
    "                    \n",
    "                if self.out_idx  in self.topk_doc:\n",
    "                    self.MRR += 1/ (list(self.topk_doc).index(self.out_idx ) + 1)\n",
    "                    \n",
    "        print(\"HR_10 \",self.HR_10 / len(self.offset_sessions))\n",
    "        print(\"HR_5 \",self.HR_5 / len(self.offset_sessions))\n",
    "        print(\"HR_1 \",self.HR_1 / len(self.offset_sessions))   \n",
    "        print(\"MRR_10\", self.MRR / len(self.offset_sessions))   \n",
    " \n",
    "                \n",
    "    def fit(self, verbose=False):\n",
    "\n",
    "        def sigmoid(input_val):\n",
    "            return 1 / (1 + (math.e ** -input_val))\n",
    "\n",
    "        self.error_during_train = False\n",
    "        self.sess.run(self.item_id_hash_table.insert(self.ItemIdList, self.ItemIdxList))\n",
    "        self.sess.run(self.item_id_hash_table_reverse.insert(self.ItemIdxList, self.ItemIdList))\n",
    "        \n",
    "        \n",
    "        max_test_auc = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            \n",
    "\n",
    "            self.offset_sessions = np.zeros(self.data['SessionId'].nunique() + 1, dtype=np.int32)\n",
    "            self.offset_sessions[1:] = self.data.groupby('SessionId').size().cumsum()\n",
    "\n",
    "            cost_train_list = []\n",
    "            cost_test_list = []\n",
    "            cut_cost_test_list = []\n",
    "\n",
    "            auc_train_list = []\n",
    "            auc_test_list = []\n",
    "            cut_auc_test_list = []\n",
    "            \n",
    "            session_idx_arr = np.arange(len(self.offset_sessions) - 1)\n",
    "                \n",
    "            iters = np.arange(self.batch_size_pos)\n",
    "            maxiter = iters.max()  # self.batch_size - 1\n",
    "            # iters is a array, so start will return a array with size of batch_size\n",
    "            start = self.offset_sessions[session_idx_arr[iters]]\n",
    "            end = self.offset_sessions[session_idx_arr[iters] + 1] \n",
    "            \n",
    "            \n",
    "            finished = False\n",
    "            \n",
    "            tolerant_time = self.tolerant_time\n",
    "\n",
    "            train_global_step = 0\n",
    "            step = 0\n",
    "\n",
    "            in_idx_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int)\n",
    "            in_tag_matrix = np.zeros([self.batch_size_pos, self.sequence_length * self.max_text_len], dtype=int)\n",
    "            in_time_diff_matrix = np.zeros([self.batch_size_pos, self.sequence_length], dtype=int) + 28\n",
    "\n",
    "            while not finished:\n",
    "                minlen = (end - start).min()\n",
    "                \n",
    "                out_idx = self.data.ItemId.values[start]\n",
    "                #out_tag = self.data.TagId.values[start]\n",
    "                out_sessionId = self.data.SessionId.values[start]\n",
    "                out_time = self.data.Time.values[start]\n",
    "                \n",
    "                ###\n",
    "                for i in range(minlen - 1):\n",
    "\n",
    "                    pos_idx = start + i\n",
    "                    in_idx = self.data.ItemId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    in_tag = self.data.tags.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    \n",
    "                    \n",
    "                    in_tag_list = np.asarray([in_tag_i.split(' ') for in_tag_i in in_tag])\n",
    "                    in_tag = in_tag_list.astype(np.int)\n",
    "                    \n",
    "                    in_sessionId = self.data.SessionId.values[pos_idx].tolist()  # * (1 + self.neg_pos_sam_rat)\n",
    "                    \n",
    "#                     print(\"in_idx\", in_idx)\n",
    "#                     print(\"pos_idx\", pos_idx)\n",
    "#                     print(\"in_sessionId\", in_sessionId)\n",
    "#                     break\n",
    "                    \n",
    "                    in_um_tip_id_list = [self.data['tips'][posid] for posid in pos_idx]\n",
    "                    in_um_tip_id = np.asarray([in_tip_i.split(' ') for in_tip_i in in_um_tip_id_list])\n",
    "                    in_um_tip_id = in_um_tip_id.astype(np.int)\n",
    "                    #self.in_um_tip_id = in_um_tip_id\n",
    "\n",
    "                    train_idx = start + i + 1\n",
    "                    out_idx = self.data.ItemId.values[train_idx]\n",
    "\n",
    "                    # build time diff\n",
    "                    self.in_time = self.data.Time.values[pos_idx].tolist()\n",
    "                    self.out_time = self.data.Time.values[train_idx].tolist()\n",
    "                    \n",
    "                    self.in_time_diff1 = [self.out_time[k] - self.in_time[k] for k in range(len(self.out_time))]\n",
    "                    self.in_time_diff2 = [int(k % 30) for k in self.in_time_diff1]\n",
    "                    self.in_time_diff3 = [min(28, k) for k in self.in_time_diff2]\n",
    "                    self.in_time_diff = [max(0, k) for k in self.in_time_diff3]\n",
    "                \n",
    "                   \n",
    "\n",
    "                    in_idx_matrix = np.concatenate([np.reshape(np.array(in_idx), [-1, 1]), in_idx_matrix[:, 0:-1]], 1)\n",
    "                    \n",
    "                    self.in_tag = in_tag\n",
    "                    self.in_tag_matrix = in_tag_matrix\n",
    "                    \n",
    "                    #in_tag = np.expand_dims(in_tag, axis=2)\n",
    "                    in_tag_matrix = np.concatenate([in_tag, in_tag_matrix[:,0:-self.max_text_len]], 1)\n",
    "                    #in_tag_matrix = np.reshape(in_tag_matrix,[-1,self.sequence_length * self.max_text_len])\n",
    "                    \n",
    "                    self.in_idx_matrix = in_idx_matrix\n",
    "                    self.in_idx = in_idx\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    in_time_diff_matrix = np.concatenate([np.reshape(np.array(self.in_time_diff),\n",
    "                                                                     [-1, 1]), in_time_diff_matrix[:, 0:-1]], 1)\n",
    "\n",
    "                    def flip_swap_zero(matrix_in, fillval=-1):\n",
    "                        matrix_2d = []\n",
    "                        for batch_cnt in range(matrix_in.shape[0]):\n",
    "                            tmp_val = matrix_in[batch_cnt]\n",
    "                            tmp_val = np.flip(tmp_val[np.nonzero(tmp_val - fillval)], 0)\n",
    "                            matrix_2d.append(np.concatenate(\n",
    "                                [tmp_val, np.array([fillval] * (len(matrix_in[batch_cnt]) - len(tmp_val)))], 0))\n",
    "                        return matrix_2d\n",
    "                    \n",
    "                    #self.test_in_idx_matrix = in_idx_matrix\n",
    "                    #self.test_in_idx = in_idx\n",
    "                    \n",
    "                    \n",
    "                    in_idx_matrix_swap = flip_swap_zero(in_idx_matrix)\n",
    "                    in_tag_matrix_swap = flip_swap_zero(in_tag_matrix)\n",
    "                    \n",
    "                    in_time_diff_matrix_swap = flip_swap_zero(in_time_diff_matrix, fillval=28)\n",
    "\n",
    "                   \n",
    "                    \n",
    "                    num_to_select = int(len(in_sessionId) * 9.0 / 10)\n",
    "                    \n",
    "                    \n",
    "                    train_set_index = list(np.arange(num_to_select))\n",
    "                    batch_size_train = len(train_set_index)\n",
    "                    \n",
    "                    \n",
    "                    test_set_index = list(np.arange(num_to_select, len(in_sessionId)))\n",
    "                    batch_size_test = len(test_set_index)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    self.item_tmp = np.nan_to_num(in_idx_matrix_swap)[train_set_index]\n",
    "#                     self.tag_tmp = np.nan_to_num(in_tag_matrix_swap)[train_set_index]\n",
    "#                     print(\"pos_idx \",pos_idx)\n",
    "#                     print(\"np.nan_to_num(in_idx_matrix_swap)[train_set_index].shape \", np.nan_to_num(in_idx_matrix_swap)[train_set_index].shape)\n",
    "#                     print(\"in_tag.shape \", np.asarray(in_tag).shape)\n",
    "#                     print(\"in_tag_list.shape \", in_tag_list.shape)\n",
    "#                     print(\"np.nan_to_num(in_tag_list).shape \", np.nan_to_num(in_tag_list).shape)\n",
    "#                     print(\"np.nan_to_num(in_tag_list)[train_set_index].shape \", np.nan_to_num(in_tag_list)[train_set_index].shape)\n",
    "                        \n",
    "                    #print(\"np.nan_to_num(np.array(in_um_tip_id))[train_set_index]\", np.nan_to_num(np.array(in_um_tip_id))[train_set_index].shape)\n",
    "                    #print(\"np.nan_to_num(in_tag_matrix_swap)[train_set_index]\", np.nan_to_num(in_tag_matrix_swap)[train_set_index].shape)\n",
    "                    \n",
    "                    feed_dict = {self.input_item_id_tensor: np.nan_to_num(in_idx_matrix_swap)[train_set_index], \n",
    "                                 self.input_tag_id_tensor: np.nan_to_num(in_tag_matrix_swap)[train_set_index],\n",
    "                                 self.X_time_diff: np.nan_to_num(in_time_diff_matrix_swap)[train_set_index], \n",
    "                                 self.target_item_id_tensor: np.nan_to_num(np.array(out_idx))[train_set_index], \n",
    "                                 self.UM_tip_id_tensor: np.nan_to_num(np.array(in_um_tip_id))[train_set_index],\n",
    "                                 self.batch_size: batch_size_train}\n",
    "                    fetches = [self.yhat, self.cost, self.final_state, self.global_step, self.lr, self.train_op]\n",
    "                    #                     fetches = [self.trfm_out]\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    train_global_step += 1\n",
    "                    yhat, cost, state, step, lr, _ = self.sess.run(fetches, feed_dict)\n",
    "                    #                     self.trfm_out_check = self.sess.run(fetches, feed_dict)\n",
    "                    #                     print(self.trfm_out_check)\n",
    "\n",
    "                    #                     return\n",
    "                    \n",
    "\n",
    "                    # evaluate training result\n",
    "                    fetches = [self.yhat, self.cost]\n",
    "                    yhat_train, cost_train = self.sess.run(fetches, feed_dict)\n",
    "\n",
    "\n",
    "                    \n",
    "                    feed_dict_test = {self.input_item_id_tensor: np.nan_to_num(in_idx_matrix_swap)[test_set_index],\n",
    "                                      self.input_tag_id_tensor: np.nan_to_num(in_tag_matrix_swap)[test_set_index],\n",
    "                                      self.X_time_diff: np.nan_to_num(in_time_diff_matrix_swap)[test_set_index], \n",
    "                                      self.target_item_id_tensor: np.nan_to_num(np.array(out_idx))[test_set_index],\n",
    "                                      self.UM_tip_id_tensor: np.nan_to_num(np.array(in_um_tip_id))[test_set_index],\n",
    "                                      self.batch_size: batch_size_test}\n",
    "\n",
    "                    fetches_test = [self.yhat, self.cost]\n",
    "                    yhat_test, cost_test = self.sess.run(fetches_test, feed_dict_test)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    if np.isnan(cost):\n",
    "                        print(str(epoch) + ':Nan error!')\n",
    "                        self.error_during_train = True\n",
    "                        return\n",
    "\n",
    "                    cost_train_list.append(cost_train)\n",
    "\n",
    "                    label_train = np.eye(yhat_train.shape[0])\n",
    "                    self.yhat_train = yhat_train\n",
    "                    self.label_train = label_train\n",
    "                    label_1d_train = np.reshape(label_train, [-1])\n",
    "                    prob_1d_train = np.reshape(yhat_train, [-1])\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(label_1d_train, prob_1d_train, pos_label=1)\n",
    "                    auc_train = metrics.auc(fpr, tpr)\n",
    "\n",
    "                    label_test = np.eye(yhat_test.shape[0])\n",
    "                    label_1d_test = np.reshape(label_test, [-1])\n",
    "                    prob_1d_test = np.reshape(yhat_test, [-1])\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(label_1d_test, prob_1d_test, pos_label=1)\n",
    "                    auc_test = metrics.auc(fpr, tpr)\n",
    "\n",
    "                    auc_train_list.append(auc_train)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    if step % 20 == 0:\n",
    "                        #cost_test, auc_test = self.test_data()\n",
    "                        \n",
    "                        auc_test_list.append(auc_test)\n",
    "                        cost_test_list.append(cost_test)\n",
    "                        \n",
    "                        avgc = np.mean(cost_train_list)\n",
    "                        avgc_test = np.mean(cost_test_list)\n",
    "                        auc_train_avg = np.mean(auc_train_list)\n",
    "                        auc_test_avg = np.mean(auc_test_list)\n",
    "\n",
    "                        print('{} Ori_file{}, Epoch{},Step{},lr:{:.6f},loss:{:.5f},auc:{:.5f},lossTst:{:.5f},aucTst:{:.5f},MaxaucTst:{:.5f},tolerant:{:.1f}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), self.ori_file, epoch, step, lr, avgc, auc_train_avg, avgc_test, auc_test_avg, max_test_auc, tolerant_time))\n",
    "\n",
    "                        if step > 10:\n",
    "                            cost_train_list = []\n",
    "                            cost_test_list = []\n",
    "                            auc_train_list = []\n",
    "                            auc_test_list = []\n",
    "                \n",
    "                        with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                            log_file.write( '{} Ori_file{}, Epoch{},Step{},lr:{:.6f},loss:{:.5f},auc:{:.5f},lossTst:{:.5f},aucTst:{:.5f},MaxaucTst:{:.5f},tolerant:{:.1f}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), self.ori_file, epoch, step, lr, avgc, auc_train_avg, avgc_test, auc_test_avg, max_test_auc, tolerant_time))\n",
    "                            \n",
    "                            if max_test_auc > auc_test_avg:\n",
    "                                if epoch > 0:\n",
    "                                    tolerant_time -= 1\n",
    "                                if tolerant_time == 0:\n",
    "                                    finished = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                max_test_auc = auc_test_avg\n",
    "                                tolerant_time = self.tolerant_time\n",
    "                                self.saver.save(self.sess, '{}/DUIEN-model'.format(self.checkpoint_dir), global_step=epoch)\n",
    "\n",
    "                start = start + minlen - 1\n",
    "                mask = np.arange(len(iters))[(end - start) < 2]  # idx of ended sessions\n",
    "                for idx in mask:\n",
    "                    maxiter += 1\n",
    "                    if maxiter >= len(self.offset_sessions) - 1:\n",
    "                        finished = True\n",
    "                        break\n",
    "                    iters[idx] = maxiter\n",
    "                    start[idx] = self.offset_sessions[session_idx_arr[maxiter]]\n",
    "                    end[idx] = self.offset_sessions[session_idx_arr[maxiter] + 1]\n",
    "\n",
    "                if len(mask) and self.reset_after_session:\n",
    "                    in_idx_matrix[mask, :] = 0\n",
    "                    in_tag_matrix[mask, :] = 0\n",
    "                  \n",
    "                    in_time_diff_matrix[mask, :] = 28\n",
    "\n",
    "            if np.isnan(avgc):\n",
    "                print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "                with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                    log_file.write( '{} Epoch {}: Nan error!'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), epoch,\n",
    "                                                         avgc) + \"\\n\")\n",
    "                self.error_during_train = True\n",
    "                return\n",
    "            self.saver.save(self.sess, '{}/DUIEN-model'.format(self.checkpoint_dir), global_step=epoch)\n",
    "            with open(self.base_path + self.model_name + \".log\", \"a\") as log_file:\n",
    "                log_file.write('ckpt to : ' + self.checkpoint_dir)\n",
    "        '''        \n",
    "        cost_test, auc_test = self.test_data()\n",
    "        cut_auc_test_list.append(auc_test)\n",
    "        cut_cost_test_list.append(cost_test)\n",
    "        cut_avgc = np.mean(cut_cost_test_list)\n",
    "        cut_avgauc = np.mean(cut_auc_test_list)\n",
    "        print('Ori_file{}, Epoch{},cutLossTst:{:.5f},cutAucTst:{:.5f}\\n'.format(\n",
    "                                    self.ori_file, epoch, cut_avgc, cut_avgauc))\n",
    "        '''\n",
    "        _softmax_W_topN = tf.slice(self.softmax_W, [0, 0], [self.predictTopN, self.rnn_size])\n",
    "        _softmax_b_topN = tf.slice(self.softmax_b, [0], [self.predictTopN])\n",
    "\n",
    "        self._logits = tf.matmul(self.user_final, _softmax_W_topN, transpose_b=True) + _softmax_b_topN\n",
    "        # self.logits = tf.matmul(self.user_vec2, softmax_W, transpose_b=True) + softmax_b\n",
    "        self._yhat = self.final_activation(self._logits)\n",
    "\n",
    "        # ## savedModel ## #\n",
    "        self._yhat = tf.identity(self._yhat, name=\"yhat\")\n",
    "\n",
    "        # topNum = tf.divide(tf.constant(1000), tf.max(tf.constant(1), outSeqNum))\n",
    "        self._topk_prob_sample, self._topk_ind_sample = tf.nn.top_k(self._yhat, 10, name=\"topk\")\n",
    "\n",
    "        self._topk_ind_sample = self._topk_ind_sample - 1\n",
    "\n",
    "        self._topk_doc_id = self.item_id_hash_table_reverse.lookup(tf.cast(self._topk_ind_sample, tf.int64))\n",
    "\n",
    "        _topk_prob_sample_1d = tf.reshape(self._topk_prob_sample, [-1])\n",
    "        _topk_ind_sample_1d = tf.reshape(self._topk_ind_sample, [-1])\n",
    "        _topk_doc_id_1d = tf.reshape(self._topk_doc_id, [-1])\n",
    "\n",
    "        self._topk_ind_val = tf.add(_topk_ind_sample_1d, tf.constant(0), name=\"topk_ind_val\")\n",
    "        self._topk_prob_val = tf.add(_topk_prob_sample_1d, tf.constant(0.0), name=\"topk_prob_val\")\n",
    "        self._topk_doc_id = tf.identity(_topk_doc_id_1d, name=\"topk_doc_id\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del DUIEN\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)    # determines the fraction of the overall amount of memory that each visible GPU can be used\n",
    "\n",
    "#with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "args.neg_pos_sam_rat = 7\n",
    "\n",
    "#DUIEN.loaddata()\n",
    "#DUIEN.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data by pickle. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "foo:569: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "foo:577: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "DUIEN = DUIEN4Rec(sess, args)\n",
    "DUIEN.preprocess()\n",
    "DUIEN.loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "init1\n",
      "init2\n",
      "init3\n"
     ]
    }
   ],
   "source": [
    "DUIEN.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-04 12:49:16 Ori_filefoursquare_seq, Epoch0,Step20,lr:0.001000,loss:5.41343,auc:0.61943,lossTst:3.26509,aucTst:0.48822,MaxaucTst:0.00000,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:20 Ori_filefoursquare_seq, Epoch0,Step40,lr:0.001000,loss:5.32893,auc:0.64961,lossTst:3.31512,aucTst:0.49834,MaxaucTst:0.48822,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:25 Ori_filefoursquare_seq, Epoch0,Step60,lr:0.001000,loss:5.10906,auc:0.69681,lossTst:3.21497,aucTst:0.57337,MaxaucTst:0.49834,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:29 Ori_filefoursquare_seq, Epoch0,Step80,lr:0.001000,loss:4.99671,auc:0.70984,lossTst:3.21638,aucTst:0.56000,MaxaucTst:0.57337,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:33 Ori_filefoursquare_seq, Epoch1,Step100,lr:0.001000,loss:4.48266,auc:0.84431,lossTst:3.57201,aucTst:0.50822,MaxaucTst:0.57337,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:37 Ori_filefoursquare_seq, Epoch1,Step120,lr:0.001000,loss:3.76353,auc:0.90486,lossTst:3.41494,aucTst:0.61544,MaxaucTst:0.57337,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:49:42 Ori_filefoursquare_seq, Epoch1,Step140,lr:0.001000,loss:3.21012,auc:0.93142,lossTst:4.14331,aucTst:0.50036,MaxaucTst:0.61544,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:46 Ori_filefoursquare_seq, Epoch1,Step160,lr:0.001000,loss:3.43181,auc:0.90946,lossTst:3.20715,aucTst:0.66467,MaxaucTst:0.61544,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:49:51 Ori_filefoursquare_seq, Epoch2,Step180,lr:0.001000,loss:2.42537,auc:0.96381,lossTst:4.58515,aucTst:0.52018,MaxaucTst:0.66467,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:49:55 Ori_filefoursquare_seq, Epoch2,Step200,lr:0.001000,loss:1.92227,auc:0.97878,lossTst:4.41698,aucTst:0.59970,MaxaucTst:0.66467,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:49:59 Ori_filefoursquare_seq, Epoch2,Step220,lr:0.001000,loss:1.60511,auc:0.98584,lossTst:5.35410,aucTst:0.54130,MaxaucTst:0.66467,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:50:03 Ori_filefoursquare_seq, Epoch2,Step240,lr:0.001000,loss:1.80017,auc:0.97894,lossTst:4.71113,aucTst:0.60657,MaxaucTst:0.66467,tolerant:37.0\n",
      "\n",
      "2021-06-04 12:50:07 Ori_filefoursquare_seq, Epoch3,Step260,lr:0.001000,loss:1.15722,auc:0.99278,lossTst:5.67935,aucTst:0.52243,MaxaucTst:0.66467,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:50:11 Ori_filefoursquare_seq, Epoch3,Step280,lr:0.001000,loss:1.00185,auc:0.99523,lossTst:5.64170,aucTst:0.54450,MaxaucTst:0.66467,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:50:15 Ori_filefoursquare_seq, Epoch3,Step300,lr:0.001000,loss:0.84739,auc:0.99680,lossTst:4.53338,aucTst:0.66509,MaxaucTst:0.66467,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:50:19 Ori_filefoursquare_seq, Epoch3,Step320,lr:0.001000,loss:0.87343,auc:0.99588,lossTst:5.22005,aucTst:0.67201,MaxaucTst:0.66509,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:50:24 Ori_filefoursquare_seq, Epoch4,Step340,lr:0.001000,loss:0.61535,auc:0.99803,lossTst:5.87418,aucTst:0.56148,MaxaucTst:0.67201,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:50:28 Ori_filefoursquare_seq, Epoch4,Step360,lr:0.001000,loss:0.57195,auc:0.99874,lossTst:6.24720,aucTst:0.55438,MaxaucTst:0.67201,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:50:32 Ori_filefoursquare_seq, Epoch4,Step380,lr:0.001000,loss:0.50146,auc:0.99912,lossTst:6.77209,aucTst:0.57059,MaxaucTst:0.67201,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:50:36 Ori_filefoursquare_seq, Epoch4,Step400,lr:0.001000,loss:0.48493,auc:0.99911,lossTst:5.48070,aucTst:0.66367,MaxaucTst:0.67201,tolerant:37.0\n",
      "\n",
      "2021-06-04 12:50:41 Ori_filefoursquare_seq, Epoch5,Step420,lr:0.001000,loss:0.40823,auc:0.99912,lossTst:7.03444,aucTst:0.52970,MaxaucTst:0.67201,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:50:45 Ori_filefoursquare_seq, Epoch5,Step440,lr:0.001000,loss:0.36901,auc:0.99956,lossTst:6.38675,aucTst:0.60822,MaxaucTst:0.67201,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:50:49 Ori_filefoursquare_seq, Epoch5,Step460,lr:0.001000,loss:0.34645,auc:0.99959,lossTst:5.22351,aucTst:0.69893,MaxaucTst:0.67201,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:50:53 Ori_filefoursquare_seq, Epoch5,Step480,lr:0.001000,loss:0.29841,auc:0.99972,lossTst:5.78829,aucTst:0.67769,MaxaucTst:0.69893,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:50:57 Ori_filefoursquare_seq, Epoch6,Step500,lr:0.001000,loss:0.32437,auc:0.99955,lossTst:6.88470,aucTst:0.58018,MaxaucTst:0.69893,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:51:01 Ori_filefoursquare_seq, Epoch6,Step520,lr:0.001000,loss:0.26178,auc:0.99982,lossTst:5.99381,aucTst:0.60716,MaxaucTst:0.69893,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:51:05 Ori_filefoursquare_seq, Epoch6,Step540,lr:0.001000,loss:0.25847,auc:0.99979,lossTst:6.91393,aucTst:0.62142,MaxaucTst:0.69893,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:51:09 Ori_filefoursquare_seq, Epoch6,Step560,lr:0.001000,loss:0.22258,auc:0.99988,lossTst:7.14046,aucTst:0.63675,MaxaucTst:0.69893,tolerant:37.0\n",
      "\n",
      "2021-06-04 12:51:14 Ori_filefoursquare_seq, Epoch7,Step580,lr:0.001000,loss:0.28716,auc:0.99964,lossTst:8.47817,aucTst:0.51503,MaxaucTst:0.69893,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:51:18 Ori_filefoursquare_seq, Epoch7,Step600,lr:0.001000,loss:0.21909,auc:0.99983,lossTst:9.20320,aucTst:0.46160,MaxaucTst:0.69893,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:51:22 Ori_filefoursquare_seq, Epoch7,Step620,lr:0.001000,loss:0.20501,auc:0.99983,lossTst:7.68236,aucTst:0.58266,MaxaucTst:0.69893,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:51:26 Ori_filefoursquare_seq, Epoch7,Step640,lr:0.001000,loss:0.18481,auc:0.99984,lossTst:8.57807,aucTst:0.52876,MaxaucTst:0.69893,tolerant:37.0\n",
      "\n",
      "2021-06-04 12:51:30 Ori_filefoursquare_seq, Epoch8,Step660,lr:0.001000,loss:0.24590,auc:0.99968,lossTst:5.43047,aucTst:0.71817,MaxaucTst:0.69893,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:51:35 Ori_filefoursquare_seq, Epoch8,Step680,lr:0.001000,loss:0.18732,auc:0.99990,lossTst:7.50781,aucTst:0.58864,MaxaucTst:0.71817,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:51:39 Ori_filefoursquare_seq, Epoch8,Step700,lr:0.001000,loss:0.17528,auc:0.99989,lossTst:7.22511,aucTst:0.62817,MaxaucTst:0.71817,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:51:42 Ori_filefoursquare_seq, Epoch8,Step720,lr:0.001000,loss:0.16625,auc:0.99989,lossTst:8.33659,aucTst:0.57598,MaxaucTst:0.71817,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:51:47 Ori_filefoursquare_seq, Epoch9,Step740,lr:0.001000,loss:0.22813,auc:0.99979,lossTst:6.88338,aucTst:0.65101,MaxaucTst:0.71817,tolerant:40.0\n",
      "\n",
      "2021-06-04 12:51:51 Ori_filefoursquare_seq, Epoch9,Step760,lr:0.001000,loss:0.16490,auc:0.99986,lossTst:8.38204,aucTst:0.53349,MaxaucTst:0.71817,tolerant:39.0\n",
      "\n",
      "2021-06-04 12:51:55 Ori_filefoursquare_seq, Epoch9,Step780,lr:0.001000,loss:0.16490,auc:0.99989,lossTst:9.03763,aucTst:0.57574,MaxaucTst:0.71817,tolerant:38.0\n",
      "\n",
      "2021-06-04 12:51:59 Ori_filefoursquare_seq, Epoch9,Step800,lr:0.001000,loss:0.15318,auc:0.99989,lossTst:9.59533,aucTst:0.54728,MaxaucTst:0.71817,tolerant:37.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DUIEN.n_epochs = 20\n",
    "DUIEN.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR_10  0.3056768558951965\n",
      "HR_5  0.26055312954876275\n",
      "HR_1  0.14070839398350315\n",
      "MRR_10 0.19217478031160684\n"
     ]
    }
   ],
   "source": [
    "DUIEN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
